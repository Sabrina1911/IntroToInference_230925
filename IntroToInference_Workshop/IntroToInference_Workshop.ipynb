{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image Description](./images/NormalityAndInferencing.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Environment Setup\n",
        "---\n",
        "\n",
        "- Start by creating a Virtual Environment for your project\n",
        "\n",
        "Before running any code in this notebook, it's important to set up a clean Python environment to manage dependencies. We recommend using a **VENV-type virtual environment** in **Visual Studio Code (VSC)**. Follow these steps:\n",
        "\n",
        "### ‚úÖ Steps to Create a Virtual Environment in VSC\n",
        "\n",
        "1. **Open your project folder** in Visual Studio Code.\n",
        "\n",
        "2. **Open the terminal**:\n",
        "   - Go to `View` > `Terminal` or press `Ctrl + `` (backtick).\n",
        "\n",
        "3. **Create the virtual environment** by running:\n",
        "   ```bash\n",
        "   python -m venv venv\n",
        "   ```\n",
        "\n",
        "4. Once the environment is created, you need to **activate it** so that all Python packages you install are scoped to this project only.\n",
        "    On Windows:\n",
        "    ```bash\n",
        "    .\\venv\\Scripts\\activate\n",
        "    ```\n",
        "\n",
        "    On macOS/Linux\n",
        "    ```bash\n",
        "    source venv/bin/activate\n",
        "    ```\n",
        "\n",
        "5. ‚ö†Ô∏è Why Activation Matters\n",
        "Activating the virtual environment ensures that:\n",
        "\n",
        "- All package installations using pip are local to your project.\n",
        "- You avoid modifying the global Python environment, which could affect other projects or system tools.\n",
        "- Your project remains portable and reproducible, especially when sharing with others or deploying."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üì¶ Installing Required Libraries\n",
        "Once activated, install the required libraries using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Accuracy vs. Precision\n",
        "\n",
        "---\n",
        "\n",
        "![Image Description](./images/AccuracyVsPrecision.png)\n",
        "\n",
        "<div style=\"display: flex;\">\n",
        "    <div style=\"flex: 1; padding: 10px;\">\n",
        "        <h2>Accuracy</h2>\n",
        "        <p><b>Definition:</b> Accuracy refers to how close a measurement or set of measurements is to the true or accepted value of the quantity being measured. It is about correctness.</p>\n",
        "        <p><b>Target Analogy:</b> In the context of a dartboard, accuracy is how close your darts land to the bullseye.</p>\n",
        "        <p><b>Statistical Measure:</b> Accuracy is often quantified by the <b>bias</b> of a measurement. A low bias indicates high accuracy. It can also be described by the mean error.</p>\n",
        "    </div>\n",
        "    <div style=\"flex: 1; padding: 10px;\">\n",
        "        <h2>Precision</h2>\n",
        "        <p><b>Definition:</b> Precision refers to how close repeated measurements are to each other, regardless of whether they are close to the true value. It is about consistency and reproducibility.</p>\n",
        "        <p><b>Target Analogy:</b> On a dartboard, precision is how tightly clustered your darts are, even if they are far from the bullseye.</p>\n",
        "        <p><b>Statistical Measure:</b> Precision is typically quantified by the <b>spread</b> or <b>variability</b> of the measurements. Common statistical measures of precision include:</p>\n",
        "        <ul>\n",
        "            <li><b>Standard Deviation:</b> A measure of the average amount of variation or dispersion of a set of values. A smaller standard deviation indicates higher precision.</li>\n",
        "            <li><b>Variance:</b> The square of the standard deviation.</li>\n",
        "            <li><b>Range:</b> The difference between the highest and lowest values in a set of measurements.</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Normalization and Standardization\n",
        "\n",
        "\n",
        "These two terms are fundamental to the Data Exploration phase of the ML Cycle, specifically when it comes to cleaning data.\n",
        "\n",
        "![Image Description](./images/MachineLearningOperationsLifeCycle.png)\n",
        "\n",
        "### Key concepts\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td style=\"vertical-align: top; padding-right: 20px;\">\n",
        "        <img src=\"./images/NormalizationDefined.png\">\n",
        "    </td>\n",
        "    <td style=\"vertical-align: top; padding-right: 20px;\">\n",
        "        <img src=\"./images/StandardizationDefined.png\">\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "### Principles\n",
        "![Image Description](./images/NormalizationVsStandarization-1.png)\n",
        "\n",
        "### Examples\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Normalization</th>\n",
        "    <th>Standardization</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"vertical-align: top; padding-right: 20px;\">\n",
        "        <img src=\"./images/NormalizationExample-1.png\">\n",
        "    </td>\n",
        "    <td style=\"vertical-align: top; padding-right: 20px;\">\n",
        "        <img src=\"./images/StandardizationExample-1.png\">\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"vertical-align: top; padding-right: 20px;\">\n",
        "        <img src=\"./images/NormalizationExample-2.png\">\n",
        "    </td>\n",
        "    <td style=\"vertical-align: top; padding-right: 20px;\">\n",
        "        <img src=\"./images/StandardizationExample-2.png\">\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîî Normality and Probability Distributions\n",
        "\n",
        "\n",
        "Experiment with the **Discrete and Continuous** simulator. Choose the **Continuous** class and then **Normal** from the list of distributions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td style=\"vertical-align: top; padding-right: 20px;\">\n",
        "      Go to Probablity Distributions > Discrete and Contiuous. <br>\n",
        "      Then hit the <b>Continuous</b> radio. <br>\n",
        "      Then, from the pull down meny, choose <b>Normal</b>\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"vertical-align: top; padding-right: 20px;\">\n",
        "      <iframe\n",
        "        src=\"https://seeing-theory.brown.edu/probability-distributions/index.html?section2\"\n",
        "        width=\"800\"\n",
        "        height=\"400\">\n",
        "      </iframe>\n",
        "    </td>\n",
        "    <td style=\"width: 200px; height: 400px\">\n",
        "      <p>\n",
        "        When the mean (Œº) is greater than 0 and the variance (œÉ¬≤) is zero, all the data points are tightly clustered.<br><br>\n",
        "        When the mean (Œº) is greater than 0 and the variance (œÉ¬≤) is very large, the data points are spread far apart.\n",
        "      </p>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Z-distribution\n",
        "\n",
        "The standard normal distribution has **mean Œº = 0** and **standard deviation œÉ = 1**.   \n",
        "It is used to standardize data and calculate probabilities.  \n",
        "Z-scores show how many standard deviations a value is from the mean.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td style=\"vertical-align: top; padding-right: 20px;\">\n",
        "      <strong>Steps to Use the Simulator</strong><br><br>\n",
        "      1. Open the simulator below.<br>\n",
        "      2. Adjust the Z‚ÇÅ and Z‚ÇÇ sliders to set the bounds.<br>\n",
        "      3. Observe how the shaded area changes.<br>\n",
        "      4. Use the area value to interpret probability.<br>\n",
        "      5. Try different values to explore the distribution.\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"vertical-align: top; padding-right: 20px;\">\n",
        "      <iframe\n",
        "        src=\"https://www.geogebra.org/m/zeF3hkXf\"\n",
        "        width=\"800\"\n",
        "        height=\"1000\">\n",
        "      </iframe>\n",
        "    </td>\n",
        "    <td  style=\"vertical-align: top; padding-right: 20px; width: 100; height: 800;\">\n",
        "      <p>\n",
        "        Z‚ÇÅ and Z‚ÇÇ define the lower and upper bounds on the standard normal curve.  \n",
        "        The shaded area between them represents the probability of a value falling within that range.  \n",
        "        A larger area means higher likelihood; a smaller area means lower probability.\n",
        "      </p>\n",
        "      <p>\n",
        "        The Z-score measures how many standard deviations a data point is from the mean of a dataset. It helps identify outliers and compare values across different distributions. A Z-score of 0 means the value is exactly at the mean, while positive or negative scores indicate how far above or below the mean the value lies.\n",
        "      </p>\n",
        "      <p>\n",
        "üìê Z-Score\n",
        "The Z-score measures how many standard deviations a data point is from the mean.  \n",
        "It helps identify outliers and compare values across different distributions.  \n",
        "A Z-score of 0 means the value is at the mean; positive or negative scores show how far above or below the mean the value lies.\n",
        "\n",
        "**Formula:**\n",
        "$$\n",
        "Z = \\frac{X - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- Z is the z-score,\n",
        "- X is the value of the element,\n",
        "- Œº is the population mean, and\n",
        "- œÉ is the standard deviation.\n",
        "\n",
        "**Example:**  \n",
        "In a class where the average score is 70 and the standard deviation is 10,  \n",
        "a student scoring 85 has:\n",
        "\n",
        "$$\n",
        "Z = \\frac{85 - 70}{10} = 1.5\n",
        "$$\n",
        "This means the score is 1.5 standard deviations above the mean.\n",
        "      </p>\n",
        "      <p>\n",
        "        Example: In a class where the average test score is 70 with a standard deviation of 10, a student scoring 85 has a Z-score of (85 - 70) / 10 = 1.5.This means the score is 1.5 standard deviations above the mean.\n",
        "      </p>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìê Z-Scores: Proportions & Reverse Lookups (Example: Student Score = 85)\n",
        "\n",
        "\n",
        "When we have a normal distribution (not standardized), say with mean $\\mu$ and standard deviation $\\sigma$, we often want to answer questions like:\n",
        "\n",
        "> *‚ÄúWhat proportion (or percentage) of students score less than $X = 85$?‚Äù*\n",
        "\n",
        "This is where **standardization** with Z-scores comes in.\n",
        "\n",
        "\n",
        "#### 1. Standardization: Transforming to the Standard Normal\n",
        "\n",
        "The standard normal distribution has:\n",
        "\n",
        "- mean $0$ (denoted $\\mu = 0$),\n",
        "- standard deviation $1$ (denoted $\\sigma = 1$).\n",
        "\n",
        "We convert any normal variable $X$ with mean $\\mu$ and standard deviation $\\sigma$ into a **Z-score** via:\n",
        "\n",
        "$$\n",
        "Z = \\frac{X - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "This transformation shifts and scales the variable so we can use standard normal tables or functions.\n",
        "\n",
        "\n",
        "#### 2. Example: Proportion Less Than a Value\n",
        "\n",
        "Suppose:\n",
        "\n",
        "- $X$ = test scores,\n",
        "- $\\mu = 70$,\n",
        "- $\\sigma = 10$,\n",
        "- We want to find the proportion of students scoring **less than** $X = 85$.\n",
        "\n",
        "**Step A.** Compute the Z-score for $X = 85$:\n",
        "\n",
        "$$\n",
        "Z = \\frac{85 - 70}{10} = \\frac{15}{10} = 1.5\n",
        "$$\n",
        "\n",
        "**Step B.** Ask: *What is $P(X < 85)$?*  \n",
        "Because $X < 85 \\;\\;\\Longleftrightarrow\\;\\; Z < 1.5$.\n",
        "\n",
        "We can look up $P(Z < 1.5)$ in a standard normal table or use a calculator / programming language (e.g. `scipy.stats.norm.cdf(1.5)`).\n",
        "\n",
        "That value is about:\n",
        "\n",
        "$$\n",
        "P(Z < 1.5) \\approx 0.9332\n",
        "$$\n",
        "\n",
        "So about **93.32%** of students are expected to score less than 85.\n",
        "\n",
        "\n",
        "#### 3. Reverse Lookup: From Proportion to Value\n",
        "\n",
        "Sometimes we want the opposite: *‚ÄúWhat test score corresponds to the 95th percentile?‚Äù* In other words:\n",
        "\n",
        "> Find $x$ such that $P(X < x) = 0.95$.\n",
        "\n",
        "**Step A.** In standard normal world, find $z$ such that $P(Z < z) = 0.95$.  \n",
        "From tables or functions, $z \\approx 1.645$ (or 1.64 depending on precision).\n",
        "\n",
        "**Step B.** Turn that back into the original $X$ scale by solving:\n",
        "\n",
        "$$\n",
        "z = \\frac{x - \\mu}{\\sigma} \n",
        "\\;\\;\\Longrightarrow\\;\\; \n",
        "x = \\mu + z \\sigma\n",
        "$$\n",
        "\n",
        "With $\\mu = 70, \\sigma = 10$:\n",
        "\n",
        "$$\n",
        "x = 70 + (1.645)(10) = 70 + 16.45 = 86.45\n",
        "$$\n",
        "\n",
        "So the 95th percentile score is about **86.45**.\n",
        "\n",
        "\n",
        "#### 4. Why This Matters\n",
        "\n",
        "- Any normal distribution, no matter its mean or standard deviation, can be converted into the standard normal via standardization.  \n",
        "- This lets us use one table or one set of tools (cdf, quantile) instead of hav\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üé• Explore Standardization Further\n",
        "\n",
        "Want to deepen your understanding of **standardization** and how Z-scores are used to calculate probabilities?\n",
        "\n",
        "Check out this short video lesson:\n",
        "\n",
        "üëâ [Watch on YouTube: Standardizing Normal Distributions](https://www.youtube.com/watch?v=2tuBREK_mgE)\n",
        "\n",
        "The video walks you through:\n",
        "\n",
        "- Why we standardize normal distributions,  \n",
        "- How any normal distribution can be converted to the **standard normal** ($\\mu = 0$, $\\sigma = 1$),  \n",
        "- Using Z-scores to find proportions (areas under the curve),  \n",
        "- Doing reverse lookups (from a proportion to a Z-score).  \n",
        "\n",
        "Take notes as you watch, and think about how the examples connect to the practice problems in this notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üé• Explore Normalization Further\n",
        "\n",
        "Want to deepen your understanding of **normalization** and how to **normalize raw data**.\n",
        "\n",
        "Check out this short video lesson:\n",
        "\n",
        "üëâ [Watch on YouTube: Min-Max Normalization](https://youtu.be/-LC_PKBoZfk?si=axDsYAqT6fKs-rJ-)\n",
        "\n",
        "The video walks you through:\n",
        "\n",
        "- Min-Max Normalization with a numerical example,  \n",
        "- Normalization with a numerical example,  \n",
        "\n",
        "Take notes as you watch, and think about how the examples connect to the practice problems in this notebook. \n",
        "\n",
        "We will continue to use normalization during the remainder of the workshop (below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù In-Class Activity: Stock Market Investment and Z-Scores\n",
        "\n",
        "You are analyzing the daily returns of a particular stock.  \n",
        "You know the following information:\n",
        "\n",
        "- The stock‚Äôs mean daily return ($\\mu$) is **0.5%**.  \n",
        "- The daily returns‚Äô standard deviation ($\\sigma$) is **2%**.  \n",
        "\n",
        "You want to find out how unusual a **daily return of 5%** is.\n",
        "\n",
        "\n",
        "#### Your Task\n",
        "\n",
        "1. **Write down the Z-score formula**:\n",
        "\n",
        "   $$\n",
        "   Z = \\frac{X - \\mu}{\\sigma}\n",
        "   $$\n",
        "\n",
        "2. **Substitute the values** into the formula using:\n",
        "   - $X = 5$ (the daily return in %),\n",
        "   - $\\mu = 0.5$,\n",
        "   - $\\sigma = 2$.\n",
        "\n",
        "   Show your calculation for $Z$.\n",
        "\n",
        "3. **Interpret the Z-score**:  \n",
        "   - What does your calculated Z-score mean in terms of how many standard deviations the 5% return is from the mean?  \n",
        "   - Is this return unusually high compared to the average?\n",
        "\n",
        "4. **Find the probability**:  \n",
        "   - Use the Z-score you found to determine the cumulative probability $P(X \\leq 5)$.  \n",
        "   - You can use a Z-table or a Python function such as:\n",
        "\n",
        "     ```python\n",
        "     from scipy.stats import norm\n",
        "     norm.cdf(Z_value)\n",
        "     ```\n",
        "\n",
        "5. **Interpret the probability**:  \n",
        "   - What percentage of daily returns fall below 5%?  \n",
        "   - What percentage of daily returns exceed 5%?\n",
        "\n",
        "\n",
        "#### üîé Reflection Questions\n",
        "- Why is a Z-score useful when comparing returns to the average?  \n",
        "- If the standard deviation were **larger**, how would that affect the Z-score for the same return of 5%?  \n",
        "\n",
        "\n",
        "### üíª **Now it‚Äôs your turn to code!**  \n",
        "Write a short Python script that:  \n",
        "- Calculates the Z-score for $X = 5$,  \n",
        "- Uses `scipy.stats.norm.cdf()` to compute the probability,  \n",
        "- Prints both results clearly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Z-score for a 5% return: 2.25\n",
            "Probability of return ‚â§ 5%: 0.9878 (98.78%)\n",
            "Probability of return > 5%: 0.0122 (1.22%)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Write the code here\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Given values\n",
        "X = 5       # daily return in %\n",
        "mu = 0.5    # mean return in %\n",
        "sigma = 2   # standard deviation in %\n",
        "\n",
        "# 1. Calculate Z-score\n",
        "Z = (X - mu) / sigma\n",
        "\n",
        "# 2. Compute cumulative probability P(X ‚â§ 5)\n",
        "prob_below = norm.cdf(Z)         # probability of ‚â§ 5%\n",
        "prob_above = 1 - prob_below      # probability of > 5%\n",
        "\n",
        "# Print results\n",
        "print(f\"Z-score for a 5% return: {Z:.2f}\")\n",
        "print(f\"Probability of return ‚â§ 5%: {prob_below:.4f} ({prob_below*100:.2f}%)\")\n",
        "print(f\"Probability of return > 5%: {prob_above:.4f} ({prob_above*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### üí≠ Reflection: Add Your Talking Point\n",
        "\n",
        "---\n",
        "Z-scores are useful because they standardize values and let us compare results across different datasets.  \n",
        "I learned that a 5% return is much higher than normal because most daily returns stay close to 0.5%.  \n",
        "Almost all returns are below 5%, and only a very small chance ie; 1.22% goes higher.  \n",
        "I also realized that if the stock moved up and down more often, a 5% return would not feel as rare.  \n",
        "---\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Statistical Inference: From Sample to Population\n",
        "\n",
        "**Statistical inference** is the act of generalizing from a **sample** to a **population** with a calculated degree of certainty.\n",
        "\n",
        "- We want to learn about **population parameters** ‚Ä¶  \n",
        "- ‚Ä¶but we can only calculate **sample statistics**.\n",
        "\n",
        "\n",
        "#### Example: Stock Market Context\n",
        "\n",
        "- **Population** = Stock Market Performance  \n",
        "- **Sample (Data)** = Company XYZ Stock  \n",
        "- **Statistic** = Z-Score (calculated from the sample)  \n",
        "- **Parameter** = Probability of daily return  \n",
        "\n",
        "We use the **sample statistic** (Z-score from XYZ stock) to make an **inference** about the **population parameter** (probability of daily return across the market).\n",
        "\n",
        "![Image Description](./images/InferenceFromSample.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üåç Challenge #1: Climate Change and Z-Scores\n",
        "\n",
        "Let‚Äôs consider an example where you are analyzing **annual temperature anomalies** to study climate change.\n",
        "\n",
        "Suppose you have collected data on the annual temperature anomalies (differences from the long-term average temperature) for a particular region over the past 30 years.\n",
        "\n",
        "- Mean annual temperature anomaly ($\\mu$) = **0.5¬∞C**  \n",
        "- Standard deviation ($\\sigma$) = **0.2¬∞C**  \n",
        "\n",
        "You want to understand how unusual a year with a **temperature anomaly of 0.9¬∞C** is.\n",
        "\n",
        "\n",
        "#### üß© Your Task\n",
        "\n",
        "1. **Use the Z-score formula**:\n",
        "\n",
        "   $$\n",
        "   Z = \\frac{X - \\mu}{\\sigma}\n",
        "   $$\n",
        "\n",
        "2. **Substitute the values** into the formula using:\n",
        "   - $X = 0.9$,  \n",
        "   - $\\mu = 0.5$,  \n",
        "   - $\\sigma = 0.2$.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "   3. üíª Code the Climate Change Z-Score in Python <br>\n",
        "      - Complete the Python code scaffold below.  \n",
        "      - Calculate the Z-score.  \n",
        "      - Use `scipy.stats.norm.cdf()` to compute the cumulative probability $P(X \\leq 0.9)$.  \n",
        "      - Interpret the result:  \n",
        "         - What percentage of years have anomalies less than or equal to 0.9¬∞C?  \n",
        "         - What percentage have anomalies greater than 0.9¬∞C?  \n",
        "\n",
        "```python\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Given values\n",
        "mu = 0.5   # mean anomaly\n",
        "sigma = 0.2  # standard deviation\n",
        "X = 0.9   # observed anomaly\n",
        "\n",
        "# 1. Compute the Z-score\n",
        "Z = ( z - mu ) / sigma\n",
        "print(\"Z-score:\", round(Z, 2))\n",
        "\n",
        "# 2. Compute the cumulative probability\n",
        "p_less = norm.cdf(Z)\n",
        "print(\"P(X <= 0.9):\", f\"{p_less:.4f}\", f\"({p_less*100:.2f}%)\")\n",
        "\n",
        "# 3. Compute the probability above 0.9\n",
        "p_greater = 1 - p_less\n",
        "print(\"P(X > 0.9):\", f\"{p_greater:.4f}\", f\"({p_greater*100:.2f}%)\")\n",
        "````\n",
        "\n",
        "      - Write the complete object-oriened Python code the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Z-score: 2.00\n",
            "P(X ‚â§ 0.9¬∞C): 0.9772 (97.72%)\n",
            "P(X > 0.9¬∞C): 0.0228 (2.28%)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Write the code here\n",
        "from dataclasses import dataclass\n",
        "from scipy.stats import norm\n",
        "\n",
        "@dataclass\n",
        "class ClimateAnomalyAnalyzer:\n",
        "    \"\"\"Analyze temperature anomalies with Z-scores and probabilities.\"\"\"\n",
        "    mu: float     # mean anomaly\n",
        "    sigma: float  # standard deviation\n",
        "\n",
        "    def z_score(self, x: float) -> float:\n",
        "        if self.sigma <= 0:\n",
        "            raise ValueError(\"Standard deviation (sigma) must be positive\")\n",
        "        return (x - self.mu) / self.sigma\n",
        "\n",
        "    def prob_leq(self, x: float) -> float:\n",
        "        \"\"\"Return P(X <= x).\"\"\"\n",
        "        return norm.cdf(self.z_score(x))\n",
        "\n",
        "    def prob_gt(self, x: float) -> float:\n",
        "        \"\"\"Return P(X > x).\"\"\"\n",
        "        return 1 - self.prob_leq(x)\n",
        "\n",
        "    def report(self, x: float) -> str:\n",
        "        z = self.z_score(x)\n",
        "        p_leq = self.prob_leq(x)\n",
        "        p_gt = self.prob_gt(x)\n",
        "        return (\n",
        "            f\"Z-score: {z:.2f}\\n\"\n",
        "            f\"P(X ‚â§ {x:.1f}¬∞C): {p_leq:.4f} ({p_leq*100:.2f}%)\\n\"\n",
        "            f\"P(X > {x:.1f}¬∞C): {p_gt:.4f} ({p_gt*100:.2f}%)\"\n",
        "        )\n",
        "\n",
        "# Example usage\n",
        "analyzer = ClimateAnomalyAnalyzer(mu=0.5, sigma=0.2)\n",
        "print(analyzer.report(0.9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After you have written and tested the code:\n",
        "\n",
        "4.  üìà Visualization Exercise: Climate Anomaly Probability **Interpret the Z-score**:  \n",
        "   - How many standard deviations is 0.9¬∞C above the mean?  \n",
        "   - Is this anomaly unusually high compared to the average year?  \n",
        "\n",
        "   - Let‚Äôs make the result more visual. We‚Äôll plot the normal distribution curve for annual temperature anomalies and shade the probability of having a value **greater than 0.9¬∞C**.\n",
        "      - Run the code below.  \n",
        "      - Observe the shaded region above $X = 0.9$.  \n",
        "      - Compare the shaded probability with the number you calculated earlier using the Z-score.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Given values\n",
        "mu = 0.5   # mean anomaly\n",
        "sigma = 0.2  # standard deviation\n",
        "X = 0.9   # observed anomaly\n",
        "\n",
        "# Generate x values for the curve\n",
        "x = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\n",
        "y = norm.pdf(x, mu, sigma)\n",
        "\n",
        "# Plot the normal distribution curve\n",
        "plt.plot(x, y, label=\"Normal Distribution\", linewidth=2)\n",
        "\n",
        "# Shade the region above X = 0.9\n",
        "x_fill = np.linspace(X, mu + 4*sigma, 500)\n",
        "y_fill = norm.pdf(x_fill, mu, sigma)\n",
        "plt.fill_between(x_fill, y_fill, alpha=0.5)\n",
        "\n",
        "# Add vertical line at X = 0.9\n",
        "plt.axvline(X, color=\"red\", linestyle=\"--\", label=f\"X = {X}\")\n",
        "\n",
        "# Labels and legend\n",
        "plt.title(\"Probability of Annual Temperature Anomaly > 0.9¬∞C\")\n",
        "plt.xlabel(\"Temperature Anomaly (¬∞C)\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üöÄ Challenge #2: Integrating Z-Scores, Visualization, and Web Services\n",
        "\n",
        "You‚Äôve now completed two separate tasks:  \n",
        "1. **Coding the Climate Change Z-Score**: You calculated the Z-score and probabilities using Python code.  \n",
        "2. **Visualizing the Climate Anomaly Probability**: You plotted the normal curve and shaded the probability region above $X = 0.9$.\n",
        "\n",
        "Both tasks are useful on their own, but in real Data Science work we want our tools to be **reusable, organized, and accessible to others**.\n",
        "\n",
        "### üß© Your Task\n",
        "\n",
        "Step 1. Combine Both Codes Using Object-Oriented Python\n",
        "- Create a Python **class** called `ClimateAnomalyAnalyzer`.  \n",
        "- The class should:\n",
        "  - Store the values of $\\mu$, $\\sigma$, and $X$ as attributes.  \n",
        "  - Have a method `compute_zscore()` that calculates and returns the Z-score.  \n",
        "  - Have a method `compute_probabilities()` that calculates and returns $P(X \\leq X)$ and $P(X > X)$.  \n",
        "  - Have a method `plot_distribution()` that produces the visualization you created earlier (curve + shaded area).  \n",
        "\n",
        "\n",
        "Step 2. Build a Flask Web Service\n",
        "- Create a **Flask app** that exposes a web service so that users can interact with your analysis.  \n",
        "- The app should:\n",
        "  - Provide a **form** or **query parameters** where the user enters values for $\\mu$, $\\sigma$, and $X$.  \n",
        "  - Display the **calculated Z-score** and **probabilities**.  \n",
        "  - Show the **distribution plot** as an image.  \n",
        "\n",
        "*(Hint: you can save the Matplotlib plot as a `.png` file in memory and serve it in the Flask app.)*\n",
        "\n",
        "\n",
        "Step 3. Test Your Web Service\n",
        "Building a web service is only half the job ‚Äî the other half is **testing it like a user would**. There are three tests for you to implement: \n",
        "\n",
        "> **Browser Test (quick check):**  \n",
        "   - Run your Flask app with `python app.py`.  \n",
        "   - Open a browser and visit:  \n",
        "     ```\n",
        "     http://127.0.0.1:5000/analyze?mu=0.5&sigma=0.2&X=0.9\n",
        "     ```  \n",
        "   - You should see a JSON response or an HTML page with results.  \n",
        "\n",
        "> **Command Line Test with `curl`:**  \n",
        "   - Run:  \n",
        "     ```bash\n",
        "     curl \"http://127.0.0.1:5000/analyze?mu=0.5&sigma=0.2&X=0.9\"\n",
        "     ```  \n",
        "   - This checks that the service responds correctly to HTTP requests.  \n",
        "\n",
        "> **Python Test with `requests` library:**  \n",
        "   ```python\n",
        "   import requests\n",
        "\n",
        "   response = requests.get(\n",
        "       \"http://127.0.0.1:5000/analyze\",\n",
        "       params={\"mu\": 0.5, \"sigma\": 0.2, \"X\": 0.9}\n",
        "   )\n",
        "   print(response.json())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 4. Reflection\n",
        "\n",
        "Explain how using Object-Oriented Python made your code cleaner and easier to extend.\n",
        "\n",
        "Reflect on how wrapping your analysis in a Flask web service could make it accessible to others, e.g. policymakers, researchers, or classmates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### üí≠ Reflection: Add Your Talking Point\n",
        "\n",
        "---\n",
        "TODO: Your reflection goes here\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä T-Score\n",
        "\n",
        "The **t-score** is a type of standard score used when the **sample size is small** or when the **population standard deviation ($\\sigma$) is unknown**.  \n",
        "\n",
        "It works very much like the **z-score**, but with one important difference:  \n",
        "- Instead of using the population standard deviation ($\\sigma$), it uses the **sample standard deviation ($s$)**.  \n",
        "- It also adjusts for the **sample size ($n$)**, since smaller samples add more uncertainty.\n",
        "\n",
        "\n",
        "### Formula\n",
        "\n",
        "$$\n",
        "T = \\frac{X - \\mu}{s / \\sqrt{n}}\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- $T$ = t-score  \n",
        "- $X$ = value of the observation  \n",
        "- $\\mu$ = sample mean  \n",
        "- $s$ = sample standard deviation  \n",
        "- $n$ = sample size  \n",
        "\n",
        "\n",
        "### Why use the t-score?\n",
        "- When $n$ is **large** and $\\sigma$ is known ‚Üí use the **z-score**.  \n",
        "- When $n$ is **small** or $\\sigma$ is unknown ‚Üí use the **t-score**.  \n",
        "\n",
        "The t-distribution is wider and has heavier tails than the normal distribution, reflecting more uncertainty with smaller samples. As $n$ grows larger, the t-distribution approaches the standard normal distribution.\n",
        "\n",
        "\n",
        "### üìù Exercise\n",
        "\n",
        "You collected the following data on exam scores from a **small sample of $n = 10$ students**:\n",
        "\n",
        "- Sample mean ($\\mu$) = 75  \n",
        "- Sample standard deviation ($s$) = 8  \n",
        "- One student scored $X = 90$  \n",
        "\n",
        "**Task:**  \n",
        "1. Calculate the t-score for the student‚Äôs score of 90.  \n",
        "2. Interpret the result: how many standard errors above the sample mean is this student‚Äôs score?  \n",
        "3. Discuss: why might using the z-score here be misleading?  \n",
        "\n",
        "üíª *Challenge:* Write Python code using `scipy.stats.t.cdf()` to calculate the probability of observing a score at least this extreme with $n-1$ degrees of freedom.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
            " * Running on http://127.0.0.1:5000\n",
            "Press CTRL+C to quit\n",
            " * Restarting with stat\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "1",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sabrina\\1557_VSC\\DataAnalysis\\CW_230925\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Write the code here\n",
        "# ALL-IN-ONE: OOP + Visualization + Flask Service\n",
        "# -----------------------------------------------\n",
        "# Requirements (in your .venv):\n",
        "#   pip install numpy matplotlib scipy flask requests\n",
        "\n",
        "import io\n",
        "import base64\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from flask import Flask, request, jsonify, send_file, Response\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1 ‚Äî Object-Oriented API\n",
        "# ---------------------------\n",
        "class ClimateAnomalyAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyze temperature anomalies with Z-scores, probabilities,\n",
        "    and a normal-distribution plot with shaded right tail.\n",
        "    \"\"\"\n",
        "    def __init__(self, mu: float, sigma: float, X: float):\n",
        "        if sigma <= 0:\n",
        "            raise ValueError(\"sigma must be positive\")\n",
        "        self.mu = float(mu)\n",
        "        self.sigma = float(sigma)\n",
        "        self.X = float(X)\n",
        "\n",
        "    def compute_zscore(self) -> float:\n",
        "        return (self.X - self.mu) / self.sigma\n",
        "\n",
        "    def compute_probabilities(self) -> dict:\n",
        "        Z = self.compute_zscore()\n",
        "        p_leq = norm.cdf(Z)\n",
        "        p_gt = 1 - p_leq\n",
        "        return {\n",
        "            \"mu\": self.mu,\n",
        "            \"sigma\": self.sigma,\n",
        "            \"X\": self.X,\n",
        "            \"Z\": round(Z, 2),\n",
        "            \"p_leq\": float(p_leq),  # P(X ‚â§ X)\n",
        "            \"p_gt\": float(p_gt)     # P(X > X)\n",
        "        }\n",
        "\n",
        "    def make_plot(self):\n",
        "        \"\"\"\n",
        "        Returns a Matplotlib Figure with the PDF and shaded tail for X>=threshold.\n",
        "        \"\"\"\n",
        "        x = np.linspace(self.mu - 4*self.sigma, self.mu + 4*self.sigma, 600)\n",
        "        pdf = norm.pdf(x, loc=self.mu, scale=self.sigma)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 4.5))\n",
        "        ax.plot(x, pdf, label=f\"Normal PDF (Œº={self.mu}, œÉ={self.sigma})\")\n",
        "        ax.axvline(self.X, color=\"red\", linestyle=\"--\", label=f\"X = {self.X:.2f}¬∞C\")\n",
        "\n",
        "        mask = x >= self.X\n",
        "        ax.fill_between(x[mask], pdf[mask], 0, alpha=0.3,\n",
        "                        label=f\"P(X > {self.X:.2f})\")\n",
        "        ax.set_title(\"Climate Anomaly Distribution with Tail Probability\")\n",
        "        ax.set_xlabel(\"Temperature anomaly (¬∞C)\")\n",
        "        ax.set_ylabel(\"Density\")\n",
        "        ax.legend()\n",
        "        fig.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def plot_png_bytes(self) -> bytes:\n",
        "        \"\"\"Render the plot to PNG bytes (for Flask response).\"\"\"\n",
        "        fig = self.make_plot()\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format=\"png\", dpi=150, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "        buf.seek(0)\n",
        "        return buf.read()\n",
        "\n",
        "    def plot_png_base64(self) -> str:\n",
        "        \"\"\"Plot as base64 data URI (handy to embed in HTML).\"\"\"\n",
        "        png = self.plot_png_bytes()\n",
        "        return \"data:image/png;base64,\" + base64.b64encode(png).decode(\"ascii\")\n",
        "\n",
        "\n",
        "# --------------------------------\n",
        "# Step 2 ‚Äî Flask Web Service (API)\n",
        "# --------------------------------\n",
        "app = Flask(__name__)\n",
        "\n",
        "def _parse_floats():\n",
        "    try:\n",
        "        mu = float(request.args.get(\"mu\", 0.5))\n",
        "        sigma = float(request.args.get(\"sigma\", 0.2))\n",
        "        X = float(request.args.get(\"X\", 0.9))\n",
        "        return mu, sigma, X, None\n",
        "    except (TypeError, ValueError):\n",
        "        return None, None, None, jsonify({\"error\": \"mu, sigma, and X must be numeric\"}),\n",
        "\n",
        "\n",
        "@app.get(\"/analyze\")\n",
        "def analyze():\n",
        "    \"\"\"\n",
        "    Returns JSON by default.\n",
        "    - Add &format=png to get a PNG image of the plot.\n",
        "    - Add &format=html for a simple HTML page showing results + embedded plot.\n",
        "    Example:\n",
        "      /analyze?mu=0.5&sigma=0.2&X=0.9\n",
        "      /analyze?mu=0.5&sigma=0.2&X=0.9&format=png\n",
        "      /analyze?mu=0.5&sigma=0.2&X=0.9&format=html\n",
        "    \"\"\"\n",
        "    mu, sigma, X, err = _parse_floats()\n",
        "    if err:\n",
        "        return err, 400\n",
        "\n",
        "    analyzer = ClimateAnomalyAnalyzer(mu, sigma, X)\n",
        "    results = analyzer.compute_probabilities()\n",
        "    fmt = request.args.get(\"format\", \"json\").lower()\n",
        "\n",
        "    if fmt == \"png\":\n",
        "        png_bytes = analyzer.plot_png_bytes()\n",
        "        return send_file(io.BytesIO(png_bytes), mimetype=\"image/png\")\n",
        "\n",
        "    if fmt == \"html\":\n",
        "        img_b64 = analyzer.plot_png_base64()\n",
        "        html = f\"\"\"\n",
        "        <html>\n",
        "        <head><title>Climate Anomaly Analysis</title></head>\n",
        "        <body style=\"font-family: Arial, sans-serif; margin: 24px;\">\n",
        "          <h2>Climate Anomaly Analysis</h2>\n",
        "          <p><b>Inputs:</b> Œº={results['mu']}, œÉ={results['sigma']}, X={results['X']}</p>\n",
        "          <p><b>Z-score:</b> {results['Z']}</p>\n",
        "          <p><b>P(X ‚â§ X):</b> {results['p_leq']:.4f} ({results['p_leq']*100:.2f}%)<br>\n",
        "             <b>P(X &gt; X):</b> {results['p_gt']:.4f} ({results['p_gt']*100:.2f}%)</p>\n",
        "          <img src=\"{img_b64}\" alt=\"Distribution plot\" style=\"max-width: 800px; width: 100%; border: 1px solid #ddd;\" />\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "        return Response(html, mimetype=\"text/html\")\n",
        "\n",
        "    # default JSON\n",
        "    out = {\n",
        "        \"mu\": results[\"mu\"],\n",
        "        \"sigma\": results[\"sigma\"],\n",
        "        \"X\": results[\"X\"],\n",
        "        \"Z\": results[\"Z\"],\n",
        "        \"P(X <= X)\": round(results[\"p_leq\"], 4),\n",
        "        \"P(X > X)\": round(results[\"p_gt\"], 4),\n",
        "    }\n",
        "    return jsonify(out)\n",
        "\n",
        "\n",
        "# --------------------------------\n",
        "# Step 3 ‚Äî (Optional) local sanity check in notebook\n",
        "# --------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Run Flask only when executing as a script:\n",
        "    #   python app.py\n",
        "    # Tests:\n",
        "    # 1) Browser JSON:  http://127.0.0.1:5000/analyze?mu=0.5&sigma=0.2&X=0.9\n",
        "    # 2) Browser PNG:   http://127.0.0.1:5000/analyze?mu=0.5&sigma=0.2&X=0.9&format=png\n",
        "    # 3) Browser HTML:  http://127.0.0.1:5000/analyze?mu=0.5&sigma=0.2&X=0.9&format=html\n",
        "    app.run(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üè† Challenge #3: T-Scores, Housing Prices, and Degrees of Freedom\n",
        "\n",
        "Let‚Äôs revisit the example of house prices in Elmira, Ontario.  \n",
        "You collected the following sample of house prices (in thousands of dollars) from **10 houses**:\n",
        "\n",
        "$$\n",
        "[450, 470, 430, 490, 410, 460, 440, 480, 500, 455]\n",
        "$$\n",
        "\n",
        "- Sample mean ($\\bar{x}$) = 458.5 (thousand dollars)  \n",
        "- Sample standard deviation ($s$) = 25.21 (thousand dollars)  \n",
        "- Sample size ($n$) = 10  \n",
        "\n",
        "Suppose you want to determine the **T-score** for a house priced at **500 thousand dollars**.\n",
        "\n",
        "\n",
        "#### üìä Statistical Background\n",
        "\n",
        "The **t-distribution** is used when data are approximately normally distributed, but the **population variance is unknown**.  \n",
        "\n",
        "- The variance is estimated using the **degrees of freedom (df)**, defined as:\n",
        "\n",
        "$$\n",
        "df = n - 1\n",
        "$$\n",
        "\n",
        "- Why $n-1$? Because when you calculate a sample mean, one degree of freedom is \"used up\" ‚Äî the last value is constrained by the others and the mean.  \n",
        "\n",
        "For this housing dataset:\n",
        "\n",
        "$$\n",
        "df = 10 - 1 = 9\n",
        "$$\n",
        "\n",
        "\n",
        "#### üî¢ Formula for the T-Score\n",
        "\n",
        "The formula is:\n",
        "\n",
        "$$\n",
        "T = \\frac{X - \\bar{x}}{s / \\sqrt{n}}\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- $T$ = T-score  \n",
        "- $X$ = observed value (here, 500)  \n",
        "- $\\bar{x}$ = sample mean (458.5)  \n",
        "- $s$ = sample standard deviation (25.21)  \n",
        "- $n$ = sample size (10)  \n",
        "\n",
        "\n",
        "#### üìù Your Task\n",
        "\n",
        "1. **Calculate the T-score** for $X = 500$.  \n",
        "2. **Interpret the result**: how unusual is this value compared to the sample mean?  \n",
        "3. **Use the t-distribution** with $df = 9$ to calculate the probability of observing a value this high or higher.  \n",
        "   - This is a **one-tailed probability**.  \n",
        "   - Use Python‚Äôs `scipy.stats.t` distribution to do this.  \n",
        "\n",
        "\n",
        "#### üíª Python Scaffold\n",
        "\n",
        "Here‚Äôs some starter code to guide you:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import t\n",
        "\n",
        "# Given values\n",
        "x_bar = 458.5\n",
        "s = 25.21\n",
        "n = 10\n",
        "X = 500\n",
        "\n",
        "# 1. Compute the T-score\n",
        "T = ( ___ - ___ ) / ( ___ / np.sqrt(n) )\n",
        "print(\"T-score:\", T)\n",
        "\n",
        "# 2. Degrees of freedom\n",
        "df = n - 1\n",
        "print(\"Degrees of freedom:\", df)\n",
        "\n",
        "# 3. Compute one-tailed probability P(T >= observed)\n",
        "p_value = 1 - t.cdf(T, df)\n",
        "print(\"P-value (one-tailed):\", p_value)\n",
        "\n",
        "\n",
        "\n",
        "Write the code on the cell below. Don't forget to make it **Object Oriented Python**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Write the code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### üí≠ Reflection: Add Your Talking Point\n",
        "\n",
        "---\n",
        "TODO: Your reflection goes here\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìà Test for Normality\n",
        "\n",
        "Before applying many statistical methods, we need to check whether the data follow a **normal distribution**.  \n",
        "\n",
        "A **normality test** helps us decide if it is reasonable to assume the data are normally distributed.\n",
        "\n",
        "**Why Normalize a dataset?**  \n",
        "In Machine Learning, features can have very different scales (e.g., age in years vs. income in dollars).  \n",
        "If left unnormalized, algorithms that rely on distance (like K-Nearest Neighbors, k-Means, or gradient descent in neural networks) can become biased toward features with larger values.  \n",
        "\n",
        "üëâ Normalization rescales all features to the same range (commonly 0 to 1), ensuring that **each feature contributes fairly** to the model.\n",
        "\n",
        "<br/>\n",
        "\n",
        "### üîë Assumptions: Data Independence\n",
        "\n",
        "When performing a test for normality, we assume that the data are **independent**.  \n",
        "This means:\n",
        "\n",
        "1. **Independent Collection:**  \n",
        "   - Example: Water hardness data samples across the UK were collected by **source A**.  \n",
        "   - Mortality rates across the UK were collected by **source B**.  \n",
        "\n",
        "2. **No Pairing or Matching:**  \n",
        "   - The two samples (water hardness and mortality rates) are not paired or matched.  \n",
        "\n",
        "3. **No Dependence:**  \n",
        "   - The two sets of measurements do not depend on each other.  \n",
        "\n",
        "\n",
        "### üóÇÔ∏è Example Dataset\n",
        "\n",
        "We are working with the following dataset (from `water.csv`):\n",
        "\n",
        "| location | town        | mortality | hardness |\n",
        "|----------|-------------|-----------|----------|\n",
        "| South    | Bath        | 1247      | 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import the data set.\n",
        "\n",
        "#### Reading and Exporting Water Data from HSAUR Package in R\n",
        "\n",
        "The `water` dataset is originally available in the `HSAUR` package in R. To use this dataset in a Jupyter Notebook, we first need to read it into the R programming environment and then export it to a CSV file. Here are the steps involved:\n",
        "\n",
        "1. **Install and Load the HSAUR Package**: \n",
        "   - If the `HSAUR` package is not already installed, we need to install it using `install.packages(\"HSAUR\")`.\n",
        "   - Load the package using `library(HSAUR)`.\n",
        "\n",
        "2. **Read the Water Data**:\n",
        "   - The `water` dataset can be accessed directly from the `HSAUR` package using the command `data(\"water\")`.\n",
        "\n",
        "3. **Export the Data to a CSV File**:\n",
        "   - Once the data is loaded into the R environment, we can use the `write.csv()` function to export it to a CSV file. For example, `write.csv(water, \"water.csv\")` will save the dataset as `water.csv` in the current working directory.\n",
        "\n",
        "Below is the R code that performs these steps. We already executed it for you and stored the CSV file in the 'data' sub-folder.\n",
        "\n",
        "```r\n",
        "# Install and load the HSAUR package\n",
        "if(!require(HSAUR)){install.packages(\"HSAUR\")}\n",
        "library(HSAUR)\n",
        "\n",
        "# Load the water dataset\n",
        "data(\"water\")\n",
        "\n",
        "# Export the dataset to a CSV file\n",
        "write.csv(water, \"water.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import all necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in Water Data\n",
        "water = pd.read_csv(\".\\data\\water.csv\")\n",
        "water.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Display Summary Statistics on the data we just imported.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "desc_water = water.iloc[:, 2:4].describe()\n",
        "desc_water.round(2)\n",
        "\n",
        "# Additional summary\n",
        "water.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Graphical Summary of the data on water, mortality and hardness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histograms and density plots\n",
        "sns.histplot(water['mortality'], bins=12, kde=True)\n",
        "plt.xlabel('Mortality')\n",
        "plt.title('Distribution of Mortality')\n",
        "plt.show()\n",
        "\n",
        "sns.histplot(water['hardness'], bins=12, kde=True)\n",
        "plt.xlabel('Hardness')\n",
        "plt.title('Distribution of Water Hardness')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìâ Are these normal curves? Not really...\n",
        "\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### üîë Why Normalize the Data?\n",
        "\n",
        "You have two features in the `water.csv` dataset:\n",
        "\n",
        "* **Mortality** (ranges around 1200‚Äì2000)\n",
        "* **Hardness** (ranges around 5‚Äì120)\n",
        "\n",
        "These features are on **very different scales**.\n",
        "If you use them directly in **distance-based models** (like K-Nearest Neighbors) or **gradient descent‚Äìbased models** (like Logistic Regression, Neural Networks), the larger-scaled feature (**Mortality**) will dominate.\n",
        "\n",
        "üëâ This means the model may **ignore water hardness** just because mortality values are numerically larger.\n",
        "\n",
        "**Normalization** rescales the features so they‚Äôre comparable, ensuring **each feature contributes fairly** to the model.\n",
        "\n",
        "#### üß™ Demonstration in Python\n",
        "\n",
        "We‚Äôll prove this using **K-Nearest Neighbors (KNN)**, a distance-based classifier.\n",
        "I‚Äôll create a simple experiment: predict whether a town is in the **North or South** based on `mortality` and `hardness`.\n",
        "\n",
        "#### 1. Without Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Features and labels\n",
        "X = water[['mortality', 'hardness']]\n",
        "y = water['location']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "# KNN without normalization\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "print(\"Accuracy WITHOUT normalization:\", accuracy_score(y_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. With Normalization (Min-Max Scaling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize features to [0,1] range\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train/test split again (on scaled features)\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_scaled, y, random_state=42, test_size=0.3)\n",
        "\n",
        "# KNN with normalization\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train_s, y_train_s)\n",
        "y_pred_s = knn.predict(X_test_s)\n",
        "\n",
        "print(\"Accuracy WITH normalization:\", accuracy_score(y_test_s, y_pred_s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### üéØ What to Observe\n",
        "\n",
        "* The **accuracy without normalization** may be poor because the large scale of `mortality` overwhelms `hardness`.\n",
        "* After **normalization**, `hardness` is on equal footing with `mortality`, so the classifier can use **both features effectively**.\n",
        "\n",
        "üí° **Takeaway:**\n",
        "Always check the **scale of your features**. If they differ significantly, normalize (or standardize) before running experiments. Otherwise, you risk biased models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß™ Shapiro-Wilk Test: Beyond Visual Inspection\n",
        "\n",
        "When we look at histograms or density plots, we can often *guess* whether the data look approximately normal.  \n",
        "But in **machine learning workflows**, relying on **visual inspection** is not practical:\n",
        "\n",
        "- It is subjective ‚Äî two people may interpret the same plot differently.  \n",
        "- It doesn‚Äôt scale ‚Äî imagine inspecting thousands of features across hundreds of datasets.  \n",
        "\n",
        "\n",
        "#### üîë Enter the Shapiro-Wilk Test\n",
        "The **Shapiro-Wilk test** provides a **formal statistical test for normality**.\n",
        "\n",
        "- **Null hypothesis ($H_0$):** The data come from a normal distribution.  \n",
        "- **Alternative hypothesis ($H_1$):** The data do not come from a normal distribution.  \n",
        "\n",
        "It produces a **p-value**:\n",
        "- If `p > 0.05`: fail to reject $H_0$ ‚Üí data are likely normal.  \n",
        "- If `p <= 0.05`: reject $H_0$ ‚Üí data are not normal.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is a p-value anyway?\n",
        "\n",
        "üëâ A p-value tells us how surprising our data would be if nothing unusual was going on.\n",
        "\n",
        "- A small p-value means: ‚ÄúWow, this result would be very unlikely if nothing unusual was happening.‚Äù\n",
        "- A big p-value means: ‚ÄúThis result isn‚Äôt surprising at all; it could easily happen just by chance.‚Äù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üëâ A p-value is like asking: ‚ÄúIf the world was completely normal and nothing special was happening, how often would I expect to see results like this just by chance?‚Äù\n",
        "\n",
        "- If the p-value is small, it means: ‚ÄúThis result almost never happens just by chance ‚Äî maybe something real is going on.‚Äù\n",
        "- If the p-value is large, it means: ‚ÄúThis result is common enough that it could easily happen by chance ‚Äî nothing special to see here.‚Äù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üëâ Imagine you flip a fair coin. Normally, you‚Äôd expect about half heads and half tails.\n",
        "\n",
        "Now suppose you flip it 10 times and get 10 heads in a row.\n",
        "The p-value answers the question:\n",
        "\n",
        "‚ÄúIf this coin were truly fair, how likely would it be to see 10 heads in a row?‚Äù\n",
        "\n",
        "- If that probability (the p-value) is very small, you start to suspect the coin isn‚Äôt fair.\n",
        "- If it‚Äôs not that small, then the result could just be normal chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üëâ A p-value is just a number that tells you how much your result looks like it could be a random accident.\n",
        "\n",
        "- A small p-value means: ‚ÄúThis doesn‚Äôt look like an accident ‚Äî something real might be happening.‚Äù\n",
        "- A big p-value means: ‚ÄúThis looks like it could easily be an accident ‚Äî probably nothing unusual here.‚Äù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üé≤ Understanding the p-value\n",
        "\n",
        "üëâ **What is a p-value?**\n",
        "\n",
        "1. **Simplest view:**  \n",
        "   A p-value is a number that tells you how much your result looks like it could be a **random accident**.  \n",
        "   - Small p-value ‚Üí unlikely to be an accident.  \n",
        "   - Large p-value ‚Üí could easily be an accident.  \n",
        "\n",
        "2. **Everyday analogy (coin flips):**  \n",
        "   Imagine flipping a fair coin. If you get **10 heads in a row**, the p-value answers:  \n",
        "   *‚ÄúIf the coin were really fair, how likely would it be to see this result?‚Äù*  \n",
        "\n",
        "3. **Plain English version:**  \n",
        "   The p-value is the chance of seeing results **at least as extreme as yours** if nothing unusual is happening.  \n",
        "\n",
        "4. **Statistical phrasing (when we can‚Äôt avoid it):**  \n",
        "   The p-value is the probability of observing your data, or something more extreme, **assuming the null hypothesis is true**.  \n",
        "\n",
        "üí° **Takeaway:**  \n",
        "The smaller the p-value, the stronger the evidence that your result is *not* just random chance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîó Connecting p-values and the Shapiro-Wilk Test\n",
        "\n",
        "The **Shapiro-Wilk test** is a statistical test for checking whether a dataset is normally distributed.  \n",
        "It produces a **p-value**, which we interpret just like any other p-value:\n",
        "\n",
        "- **Null hypothesis ($H_0$):** The data come from a normal distribution.  \n",
        "- **Alternative hypothesis ($H_1$):** The data do not come from a normal distribution.  \n",
        "\n",
        "üëâ The p-value tells us how much the data could look like a **random accident** under the assumption of normality.  \n",
        "\n",
        "- If **p > 0.05** ‚Üí the data do not provide strong evidence against $H_0$, so it is reasonable to assume the data are normal.  \n",
        "- If **p ‚â§ 0.05** ‚Üí the data are unlikely under $H_0$, so we conclude the data are not normally distributed.  \n",
        "\n",
        "üí° In short: the **Shapiro-Wilk test uses the p-value to automate the decision** of whether a dataset is ‚Äúnormal enough‚Äù to justify using methods that assume normality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shapiro-Wilk test for normality\n",
        "shapiro_hardness = stats.shapiro(water['hardness'])\n",
        "shapiro_mortality = stats.shapiro(water['mortality'])\n",
        "print(f\"Shapiro-Wilk test for hardness: {shapiro_hardness}\")\n",
        "print(f\"Shapiro-Wilk test for mortality: {shapiro_mortality}\")\n",
        "\n",
        "# Q-Q plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Q-Q plot for hardness\n",
        "stats.probplot(water['hardness'], dist=\"norm\", plot=axes[0])\n",
        "axes[0].set_title('Q-Q Plot for Hardness')\n",
        "\n",
        "# Q-Q plot for mortality\n",
        "stats.probplot(water['mortality'], dist=\"norm\", plot=axes[1])\n",
        "axes[1].set_title('Q-Q Plot for Mortality')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öôÔ∏è Why It Matters in ML Operations\n",
        "- In automated ML pipelines, we cannot visually inspect every feature.  \n",
        "- Tests like **Shapiro-Wilk** give us a **systematic, programmatic way** to decide whether normality assumptions hold.  \n",
        "- This helps us choose the right tools:\n",
        "  - If normal ‚Üí use **parametric methods** (e.g., linear regression, t-tests).  \n",
        "  - If not normal ‚Üí consider **non-parametric methods** (e.g., Mann-Whitney test, tree-based models).  \n",
        "\n",
        "\n",
        "üëâ In short: **visual inspection is good for learning**, but **automated testing is essential for scaling machine learning operations.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Mortality and Hardness by Location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots\n",
        "sns.boxplot(x='location', y='hardness', data=water)\n",
        "plt.title('Hardness by Location')\n",
        "plt.xlabel('Regions')\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(x='location', y='mortality', data=water)\n",
        "plt.title('Mortality by Location')\n",
        "plt.xlabel('Regions')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üé• Explore p-values Further\n",
        "\n",
        "Want to deepen your understanding of **p-values** and how to **Statistical Hypothesis Tests** ?.\n",
        "\n",
        "Check out this short video lesson:\n",
        "\n",
        "üëâ [Watch on YouTube: What is a p-value?](https://youtu.be/ukcFrzt6cHk?si=tHVMt9vXkvXMWTX3)\n",
        "\n",
        "The video walks you through:\n",
        "\n",
        "- The formal definition of p-value in the context of observations,  \n",
        "- The formal definition of p-value in the context of probabilty,  \n",
        "- It's use in a drug administration use case,  \n",
        "- The concept of **Random Noise**,  \n",
        "\n",
        "Take notes as you watch, and think about how the examples connect to the practice problems in this notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üåç Challenge #4 : Country Data ‚Äî Normality & Normalization\n",
        "\n",
        "Use two free APIs about **country statistics / demographics** to fetch data, test for normality, normalize/standardize, visualize, and reflect.\n",
        "\n",
        "#### üì° Suggested APIs to Use  \n",
        "Here are two APIs that give country-level data, with numeric fields, and are free or have free endpoints:\n",
        "\n",
        "1. **REST Countries API** ‚Äî [https://restcountries.com/](https://restcountries.com/)  \n",
        "   - Returns data on countries: population, area, region, etc. :contentReference[oaicite:0]{index=0}  \n",
        "   - No API key needed.\n",
        "\n",
        "2. **World Bank API** ‚Äî [https://api.worldbank.org/v2/country](https://api.worldbank.org/v2/country)  \n",
        "   - Provides country metadata plus statistical indicators if extended, or for starters population / income etc. :contentReference[oaicite:1]{index=1}  \n",
        "   - Free to use.\n",
        "\n",
        "#### üí° Tasks\n",
        "\n",
        "1. **Fetch data** from both APIs:  \n",
        "   - For each API, get at least **100 countries**, and pick **two or more numeric features** (e.g. population, area, GDP per capita, life expectancy).  \n",
        "   - Load into DataFrames.\n",
        "\n",
        "2. **Check for normality** using the **Shapiro-Wilk test** on each numeric feature:  \n",
        "   - Compute p-values.  \n",
        "   - Note which features are *not* normal (p ‚â§ 0.05).\n",
        "\n",
        "3. **Transform (if needed)** those ‚Äúnot normal‚Äù features:  \n",
        "   - Normalize via Min-Max scaling (to range 0-1).  \n",
        "   - Standardize (z-score: mean = 0, standard deviation = 1).\n",
        "\n",
        "4. **Visualize before & after transformations**:  \n",
        "   - Use **box plots** for each feature before transformation.  \n",
        "   - Box plots after normalization.  \n",
        "   - Box plots after standardization.  \n",
        "   - Display side by side for comparison.\n",
        "\n",
        "5. **Reflection / Talking Points**: Write at least three short points about:  \n",
        "   - Why normalization/standardization was needed (or not) in your datasets.  \n",
        "   - How differences in scale showed up (outliers, spread).  \n",
        "   - Implications for using these datasets in ML models (distance-based, etc.).\n",
        "\n",
        "#### üêç Python Scaffold\n",
        "\n",
        "```python\n",
        "import requests\n",
        "import pandas as pd\n",
        "from scipy.stats import shapiro\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def test_normality(df, cols):\n",
        "    results = {}\n",
        "    for c in cols:\n",
        "        vals = df[c].dropna()\n",
        "        if len(vals) >= 3:\n",
        "            stat, p = shapiro(vals)\n",
        "            results[c] = p\n",
        "    return results\n",
        "\n",
        "# API 1: REST Countries\n",
        "url1 = \"https://restcountries.com/v3.1/all\"\n",
        "resp1 = requests.get(url1)\n",
        "data1 = pd.json_normalize(resp1.json())\n",
        "# Example numeric features\n",
        "num_cols1 = ['population', 'area']\n",
        "\n",
        "# API 2: World Bank ‚Äî get country data\n",
        "url2 = \"https://api.worldbank.org/v2/country?format=json&per_page=300\"\n",
        "resp2 = requests.get(url2)\n",
        "# World Bank returns a list of two elements: metadata and actual data\n",
        "wb_data = resp2.json()[1]  \n",
        "data2 = pd.DataFrame(wb_data)\n",
        "# Example numeric features (you might need to filter or convert)\n",
        "num_cols2 = ['population', 'longitude', 'latitude']  # or other numeric fields available\n",
        "\n",
        "# 1. Normality before transformations\n",
        "pvals1_before = test_normality(data1, num_cols1)\n",
        "pvals2_before = test_normality(data2, num_cols2)\n",
        "print(\"P-values before:\", pvals1_before, pvals2_before)\n",
        "\n",
        "# 2. Normalize / Standardize\n",
        "minmax = MinMaxScaler()\n",
        "stdscaler = StandardScaler()\n",
        "\n",
        "data1_norm = data1.copy()\n",
        "data1_norm[num_cols1] = minmax.fit_transform(data1[num_cols1])\n",
        "\n",
        "data1_std = data1.copy()\n",
        "data1_std[num_cols1] = stdscaler.fit_transform(data1[num_cols1])\n",
        "\n",
        "# 3. Normality after transformations\n",
        "pvals1_norm = test_normality(data1_norm, num_cols1)\n",
        "pvals1_std = test_normality(data1_std, num_cols1)\n",
        "print(\"P-values after Min-Max (REST Countries):\", pvals1_norm)\n",
        "print(\"P-values after Standardization (REST Countries):\", pvals1_std)\n",
        "\n",
        "# 4. Visualization\n",
        "fig, axes = plt.subplots(3, len(num_cols1), figsize=(6 * len(num_cols1), 12))\n",
        "for i, c in enumerate(num_cols1):\n",
        "    sns.boxplot(y=data1[c], ax=axes[0, i])\n",
        "    axes[0, i].set_title(f\"{c} before\")\n",
        "    sns.boxplot(y=data1_norm[c], ax=axes[1, i])\n",
        "    axes[1, i].set_title(f\"{c} normalized\")\n",
        "    sns.boxplot(y=data1_std[c], ax=axes[2, i])\n",
        "    axes[2, i].set_title(f\"{c} standardized\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## T-Test and Variance Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variance test\n",
        "res_ftest = stats.levene(water['mortality'][water['location'] == 'North'],\n",
        "                         water['mortality'][water['location'] == 'South'])\n",
        "print(f\"Variance test result: {res_ftest}\")\n",
        "\n",
        "# T-Test\n",
        "res_ttest = stats.ttest_ind(water['mortality'][water['location'] == 'North'],\n",
        "                            water['mortality'][water['location'] == 'South'],\n",
        "                            equal_var=True)\n",
        "print(f\"T-Test result: {res_ttest}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-Parametric Test for Hardness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wilcoxon test\n",
        "res_wilcox = stats.mannwhitneyu(water['hardness'][water['location'] == 'North'],\n",
        "                                water['hardness'][water['location'] == 'South'])\n",
        "print(f\"Wilcoxon test result: {res_wilcox}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot with regression line\n",
        "sns.lmplot(x='hardness', y='mortality', hue='location', data=water)\n",
        "plt.xlabel('Calcium concentration (in parts per million)')\n",
        "plt.ylabel('Averaged annual mortality per 100,000 males')\n",
        "plt.title('Comparing Water Hardness to Mortality')\n",
        "plt.show()\n",
        "\n",
        "# Pearson correlation\n",
        "pearson_corr = stats.pearsonr(water['hardness'], water['mortality'])\n",
        "print(f\"Pearson correlation: {pearson_corr}\")\n",
        "\n",
        "# Spearman correlation\n",
        "spearman_corr = stats.spearmanr(water['hardness'], water['mortality'])\n",
        "print(f\"Spearman correlation: {spearman_corr}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Categorical Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chi-Square test for water data\n",
        "# Assuming 'location' and 'mortality' are categorical for this example\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(water['location'], water['mortality'])\n",
        "\n",
        "# Chi-Square test\n",
        "chi2_test = stats.chi2_contingency(contingency_table)\n",
        "print(f\"Chi-Square test result: {chi2_test}\")\n",
        "\n",
        "# Residuals\n",
        "residuals = chi2_test[3]\n",
        "print(f\"Residuals: {residuals}\")\n",
        "\n",
        "# Graphing the residuals\n",
        "sns.heatmap(residuals, annot=True, cmap='coolwarm')\n",
        "plt.title('Residuals Heatmap')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìà Normalization, part 2\n",
        "\n",
        "### Most of the time, our raw data will not be normalized.\n",
        "\n",
        "Min-max normalization scales data to a fixed range, typically [0, 1], ensuring that no single feature dominates due to its scale. This is crucial when features have different units or ranges, as it prevents biased model performance. It also improves the efficiency of gradient-based optimization algorithms, leading to faster and more stable convergence. Additionally, normalization enhances data visualization, making it easier to compare and interpret relationships between features. Overall, min-max normalization is essential for improving the performance, efficiency, and interpretability of data analysis and machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1 - graph the raw data (not normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph the unscaled data\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.histplot(water['mortality'], bins=16, kde=True, ax=axes[0])\n",
        "axes[0].set_title('Distribution of Mortality (Unscaled)')\n",
        "axes[0].set_xlabel('Mortality')\n",
        "\n",
        "sns.histplot(water['hardness'], bins=16, kde=True, ax=axes[1])\n",
        "axes[1].set_title('Distribution of Hardness (Unscaled)')\n",
        "axes[1].set_xlabel('Hardness')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 - Apply the Min-Max scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Select the columns to normalize\n",
        "columns_to_normalize = ['mortality', 'hardness']\n",
        "\n",
        "# Apply the scaler to the selected columns\n",
        "water[columns_to_normalize] = scaler.fit_transform(water[columns_to_normalize])\n",
        "\n",
        "# Display the first few rows of the normalized dataset\n",
        "water.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 - Graph the normalized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph the scaled data\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.histplot(water['mortality'], bins=16, kde=True, ax=axes[0])\n",
        "axes[0].set_title('Distribution of Mortality (Scaled)')\n",
        "axes[0].set_xlabel('Mortality')\n",
        "\n",
        "sns.histplot(water['hardness'], bins=16, kde=True, ax=axes[1])\n",
        "axes[1].set_title('Distribution of Hardness (Scaled)')\n",
        "axes[1].set_xlabel('Hardness')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
